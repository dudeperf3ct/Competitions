{"cells":[{"metadata":{"trusted":true,"_uuid":"2477e6e37f945ed35ffb0028e83ac6514d1c5be1","collapsed":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport random\nfrom IPython.display import display\nfrom sklearn.metrics import precision_score, f1_score, recall_score, accuracy_score, roc_auc_score\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, validation_curve, cross_val_score\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.preprocessing import Imputer, StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom scipy.stats import randint, uniform\nfrom scikitplot.metrics import plot_confusion_matrix\nfrom scikitplot.estimators import plot_feature_importances\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":244,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"8f850a0f1a8ce00f9e2c503f9b48219075499c6d"},"cell_type":"code","source":"train_path = '../input/train.csv'\ntest_path = '../input/test.csv'","execution_count":245,"outputs":[]},{"metadata":{"_uuid":"c057dd20f4a16bb3d56a37fe9f6ded951a917286"},"cell_type":"markdown","source":"# Loading\n\nLoad all the necessary csv files"},{"metadata":{"trusted":true,"_uuid":"cf521583995eb0b8ed8d140c21b2015404235f69"},"cell_type":"code","source":"train_data = pd.read_csv(train_path)\ntrain_data.head()","execution_count":246,"outputs":[]},{"metadata":{"_uuid":"1341c115c54828a7fe4e3f1e3ccd02a6aca92228"},"cell_type":"markdown","source":"# Preprocessing\n\n1. Check all the dtypes of the input\n2. Check all the statistics of data (use `describe(include='all')` of pandas)\n3. Check for null values of all columns\n4. Check for data imbalance\n5. Fill the missing values for each columns \n6. Convert all the categorical input (object dtype) into int\n7. Split the data into two sets ---> train and test"},{"metadata":{"trusted":true,"_uuid":"6c3d06bfd8aa0693b82e282f0bebc7971a28db60"},"cell_type":"code","source":"train_data.info()","execution_count":247,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bce2b9981585833e80b3065026d68d432fa85f90"},"cell_type":"code","source":"train_data.describe(include='all')","execution_count":248,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c7558c9b158e54f9dc7058bedc772730ee87b48"},"cell_type":"code","source":"train_data.isnull().sum()","execution_count":249,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ebf5a849d34cc19245e95f4c112a45f48e89155"},"cell_type":"code","source":"cat_df = train_data.select_dtypes(include=['object'])\ncat_df.head()","execution_count":250,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"166a1594f6f6942f4f00aa11f6206d06b72dbdea"},"cell_type":"code","source":"plt.figure(1)\n\nplt.subplot(331)\nsns.countplot(data=train_data, x='A')\n\nplt.subplot(332)\nsns.countplot(data=train_data, x='D')\n\nplt.subplot(333)\nsns.countplot(data=train_data, x='E')\n\nplt.subplot(334)\nsns.countplot(data=train_data, x='F')\n\nplt.subplot(335)\nsns.countplot(data=train_data, x='G')\n\nplt.subplot(336)\nsns.countplot(data=train_data, x='I')\n\nplt.subplot(337)\nsns.countplot(data=train_data, x='J')\n\nplt.subplot(338)\nsns.countplot(data=train_data, x='L')\n\nplt.subplot(339)\nsns.countplot(data=train_data, x='M')\n\n# Adjust the subplot layout, because the logit one may take more space\n# than usual, due to y-tick labels like \"1 - 10^{-3}\"\nplt.subplots_adjust(top=2.0, bottom=0.08, left=0.10, right=0.95, hspace=0.5,\n                    wspace=0.5)\n\nplt.show()","execution_count":251,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c1ed288bb9799f51ed96fdf882fbe5e28931492f"},"cell_type":"code","source":"sns.countplot(x='P', data=train_data)\nplt.show()\nprint (train_data.P.value_counts())","execution_count":252,"outputs":[]},{"metadata":{"_uuid":"1cb1b836c4a9bd6bcbbb3f309c7f402f93be1b10"},"cell_type":"markdown","source":"# Handle Missing Data\n1. Easiest way is to drop the rows which contains NAN [a bit problematic if test set also contains NAN values]\n2. Fill with most frequent for categorical and median for numerical.\n3. Use the avaliable data and build a model to predict the missing values [Promising way! (how to proceed?) ]"},{"metadata":{"trusted":true,"_uuid":"e699f26856e3085949c842db12869b66b7d94707"},"cell_type":"code","source":"cols = ['A', 'D', 'E', 'F', 'G', 'I', 'J', 'L', 'M']\nfor item in cols:\n    train_data[item] = train_data[item].astype(\"category\").cat.codes +1\n\ntrain_data.head()","execution_count":253,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bff971b579fddbb191a2cbf947722fa2e5fe3fa0","collapsed":true},"cell_type":"code","source":"# cols = ['A', 'D', 'E', 'F', 'G', 'I', 'J', 'L', 'M']\n# lb = LabelEncoder()\n\n# for item in cols:\n#     train_data[item] = lb.fit_transform(train_data[item])\n\n# train_data.head()","execution_count":254,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"63c01ce63e2dc7313dc50ecfb40305a4cd390fe7"},"cell_type":"code","source":"cats = ['A', 'D', 'E', 'F', 'G']\nnot_cats = ['B', 'C', 'H', 'N']","execution_count":255,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e362eaebed0555215fa7b84536308cbbdc8d1a8c","collapsed":true},"cell_type":"code","source":"train_data.B = train_data.B.replace(0, np.nan)\ntrain_data.N = train_data.N.replace(0, np.nan)","execution_count":256,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00a0dd44869cc7ef4932c3f556da453e4274795a"},"cell_type":"code","source":"values = train_data[not_cats]\nimputer = Imputer(missing_values='NaN', strategy='mean')\ntransformed_values = imputer.fit_transform(values)\ntransformed_values = pd.DataFrame(data=transformed_values, columns=not_cats)\nprint (transformed_values.head())","execution_count":257,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d76af577402f2d3dec3ff76f73a75c69c00e16c8"},"cell_type":"code","source":"ss = StandardScaler()\nvalues = transformed_values[not_cats]\ntransformed = ss.fit_transform(values)\ntransformed = pd.DataFrame(data=transformed, columns=not_cats)\ndisplay(transformed.head())","execution_count":258,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c1f430ce66287e096483c9305d80172cd5fdadd5"},"cell_type":"code","source":"transformed_values.update(transformed)\ntransformed_values.head()","execution_count":259,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eacae26f65b57016c8a774ed0a142677ab174b74"},"cell_type":"code","source":"train_data.update(transformed_values)\nprint (len(train_data))\nimport gc\ndel transformed_values, transformed\ngc.collect()","execution_count":260,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a0694b74085f92e22956da778d39970b6a178415","collapsed":true},"cell_type":"code","source":"train_labels = train_data['P']\ntrain_data = train_data.drop(['P'], axis=1)","execution_count":261,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df2d40d8daa15b37316c5191be3727a6c4e6eabd"},"cell_type":"code","source":"train, test, y_train, y_test = train_test_split(train_data, train_labels, test_size=0.20, stratify=train_labels, random_state=42)\nprint ('Training', train.shape, y_train.shape)\nprint ('Testing', test.shape, y_test.shape)","execution_count":262,"outputs":[]},{"metadata":{"_uuid":"6325e5af04862d50b61b0e3248c029daa4dfb0ca"},"cell_type":"markdown","source":"# LightGBM"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"20295b6dba070377e1a2340bf80725bbb5452ca9"},"cell_type":"code","source":"lg = lgb.LGBMClassifier(silent=False, sigmoid=1.0)\n\n# param_dist = {\"max_depth\": [1, 2, 4, 5],\n#               \"learning_rate\" : uniform(),\n#               \"num_leaves\": [4, 15, 25],\n#               \"n_estimators\": randint(1, 1001),\n#               'lambda_l2': uniform()\n# }\n# n_iters_search = 10\n# random_search = RandomizedSearchCV(lg, n_jobs=-1, param_distributions=param_dist, cv=10, scoring=\"accuracy\", verbose=1, n_iter=n_iters_search)\n# random_search.fit(train, y_train)\n# print (random_search.best_estimator_)\n\n\ndefault_params = { 'max_depth': 6,\n                 'booster': 'dart',\n                 'max_bin': 700,\n                 'learning_rate':0.5,\n                 'num_leaves': 200,\n                 'n_estimators':250,\n                 'lambda_l2':0.05\n}\n\n\n# n_estimators_range = np.linspace(1, 200, 10).astype('int')\n# train_score, test_score = validation_curve(lgb.LGBMClassifier(**default_params),\n#                                           train_data, train_labels,\n#                                           param_name='n_estimators',\n#                                           param_range=n_estimators_range,\n#                                            cv=10,\n#                                            scoring='accuracy'\n#                                           )\n\n# train_score_mean = np.mean(train_score, axis=1)\n# test_score_mean = np.mean(test_score, axis=1)\n# fig = plt.figure(figsize=(10, 6), dpi=100)\n# plt.title('Validation Score with eta=0.3')\n# plt.xlabel('Number of trees')\n# plt.ylabel('Accuracy')\n\n# plt.plot(n_estimators_range, train_score_mean, label='Training score', color='r')\n# plt.plot(n_estimators_range, test_score_mean, label='Validation score', color='g')\n# plt.legend(loc='best')\n# plt.show()\n\ndtrain = lgb.Dataset(train, label=y_train)\ncat_features_name = ['A', 'D', 'E', 'F', 'G', 'I', 'J', 'L', 'M']\nlgb_model = lgb.train(default_params, dtrain, categorical_feature=cat_features_name)\ny_pred = lgb_model.predict(test)\ny_pred = (y_pred>0.5).astype('int')\nplot_confusion_matrix(y_test, y_pred, normalize=False)\nplt.show()\n\nprint(\"\\tPrecision: %1.3f\" % precision_score(y_test, y_pred))\nprint(\"\\tRecall: %1.3f\" % recall_score(y_test, y_pred))\nprint(\"\\tF1: %1.3f\\n\" % f1_score(y_test, y_pred))\nprint(\"\\tAccuracy: %1.3f\" % accuracy_score(y_test, y_pred))\nprint(\"\\tROC: %1.3f\" % roc_auc_score(y_test, y_pred))","execution_count":263,"outputs":[]},{"metadata":{"_uuid":"d3f5a243aa906e47f0c4f1fb28148c6be48ee21e"},"cell_type":"markdown","source":"# Xgboost\n"},{"metadata":{"trusted":true,"_uuid":"b443df664d75fa775ea84ccc4aec118f4251e34b","scrolled":true},"cell_type":"code","source":"xg = xgb.XGBClassifier(silent=False, objective='binary:logistic')\n\n# param_dist = {\"max_depth\": randint(1, 5),\n#               \"gamma\" : [0, 0.5, 1],\n#               \"n_estimators\": randint(1, 1001),\n#               \"learning_rate\": uniform(),\n#               \"colsample_bytree\": uniform(),\n#               \"subsample\":uniform(),\n#               \"reg_lambda\":uniform(),\n#               \"reg_alpha\":uniform()\n# }\n# n_iter_search = 30\n# random_search = RandomizedSearchCV(xg, n_jobs=-1, param_distributions=param_dist, cv=15, scoring=\"accuracy\", verbose=1, n_iter=n_iter_search)\n# random_search.fit(train, y_train)\n# print (random_search.best_estimator_)\n\ndefault_param = { 'objective': 'binary:logistic',\n                 'max_depth': 2,\n                 'booster':'dart',\n                 'learning_rate':0.3,\n                 'silent':1,\n                 'subsample':0.9599494371296681,\n                 'colsample_bytree':0.8679014932353589,\n                 'gamma': 0,\n                 'reg_lambda':0.45590387225044104,\n                 'reg_alpha':0.5604423471664549\n}\n\n# n_estimators_range = np.linspace(1, 300, 10).astype('int')\n# train_score, test_score = validation_curve(xgb.XGBClassifier(**default_param),\n#                                           train_data, train_labels,\n#                                           param_name='n_estimators',\n#                                           param_range=n_estimators_range,\n#                                            cv=10,\n#                                            scoring='accuracy'\n#                                           )\n\n# train_score_mean = np.mean(train_score, axis=1)\n# test_score_mean = np.mean(test_score, axis=1)\n# fig = plt.figure(figsize=(10, 6), dpi=100)\n# plt.title('Validation Score with eta=0.3')\n# plt.xlabel('Number of trees')\n# plt.ylabel('Accuracy')\n\n# plt.plot(n_estimators_range, train_score_mean, label='Training score', color='r')\n# plt.plot(n_estimators_range, test_score_mean, label='Validation score', color='g')\n# plt.legend(loc='best')\n# plt.show()\n\n#15 cv and 30 iters --> '47 3 9 52'\nxgb_model = xgb.XGBClassifier(base_score=0.5, booster='dart', colsample_bylevel=1,\n       colsample_bytree=0.770338779380999, gamma=1,\n       learning_rate=0.019096748632529414, max_delta_step=0, max_depth=1,\n       min_child_weight=1, missing=None, n_estimators=500, n_jobs=1,\n       nthread=None, objective='binary:logistic', random_state=0,\n       reg_alpha=0.2825591812041359, reg_lambda=0.07528729199726858,\n       subsample=0.43409905314113495)\n\n\n#new   --> '46 4 7 54'\n# xgb_model = xgb. XGBClassifier(base_score=0.5, booster='dart', colsample_bylevel=1,\n#        colsample_bytree=0.23382821311638435, gamma=1,\n#        learning_rate=0.03591284925507787, max_delta_step=0, max_depth=2,\n#        min_child_weight=1, missing=None, n_estimators=600, n_jobs=1,\n#        nthread=None, objective='binary:logistic', random_state=0,\n#        reg_alpha=0.01173117437539295, reg_lambda=0.02099910740625921,\n#        scale_pos_weight=1, seed=None, silent=False,\n#        subsample=0.9310717459245332)\n\n#best model\n# xgb_model = xgb.XGBClassifier(base_score=0.5, booster='dart', colsample_bylevel=1,\n#        colsample_bytree=0.7835814532405151, gamma=1,\n#        learning_rate=0.3, max_delta_step=0, max_depth=5,\n#        min_child_weight=1, missing=None, n_estimators=950, n_jobs=1,\n#        nthread=None, objective='binary:logistic', random_state=0,\n#        reg_alpha=0.08835265866227038, reg_lambda=0.9919841833494273, subsample=0.8716974299211524)\n\nxgb_model.fit(train, y_train)\ny_pred = xgb_model.predict(test)\ny_pred = (y_pred>0.5).astype('int')\nplot_confusion_matrix(y_test, y_pred, normalize=False)\nplt.show()\n\nprint(\"\\tPrecision: %1.3f\" % precision_score(y_test, y_pred))\nprint(\"\\tRecall: %1.3f\" % recall_score(y_test, y_pred))\nprint(\"\\tF1: %1.3f\\n\" % f1_score(y_test, y_pred))\nprint(\"\\tAccuracy: %1.3f\" % accuracy_score(y_test, y_pred))\nprint(\"\\tROC: %1.3f\" % roc_auc_score(y_test, y_pred))\n\n# dtrain = xgb.DMatrix(train, label=y_train)\n# dtest = xgb.DMatrix(test, label=y_test)\n# num_round = 1000\n# watchlist = [(dtrain, 'train'), (dtest, 'test')]\n# bst = xgb.train(default_param, dtrain, num_round, watchlist, early_stopping_rounds=10, maximize=False, eval_metric=\"logloss\")\n# # make prediction\n# y_pred = bst.predict(dtest)\n# y_pred = (y_pred>0.5).astype('int')\n# plot_confusion_matrix(y_test, y_pred, normalize=False)\n# plt.show()\n\n# print(\"\\tPrecision: %1.3f\" % precision_score(y_test, y_pred))\n# print(\"\\tRecall: %1.3f\" % recall_score(y_test, y_pred))\n# print(\"\\tF1: %1.3f\\n\" % f1_score(y_test, y_pred))\n# print(\"\\tAccuracy: %1.3f\" % accuracy_score(y_test, y_pred))\n# print(\"\\tROC: %1.3f\" % roc_auc_score(y_test, y_pred))\n\n'''\n45 5\n6 55\ndefault_param = { 'objective': 'binary:logistic',\n                 'max_depth': 2,\n                 'booster': 'dart',\n                 'learning_rate':0.5,\n                 'silent':1,\n                 'subsample':0.9599494371296681,\n                 'colsample_bytree':0.8679014932353589,\n                 'gamma': 1,\n                 'reg_lambda':0.45590387225044104,\n                 'reg_alpha':0.5604423471664549\nbst model\n'''","execution_count":264,"outputs":[]},{"metadata":{"_uuid":"62c43d762d76a09fe79a7508923e37d9f11c2b92"},"cell_type":"markdown","source":"# Random Forest"},{"metadata":{"trusted":true,"_uuid":"0e5b732121ad2a1001953029e42c727089747059"},"cell_type":"code","source":"import time\nfrom scipy.stats import randint as sp_randint\n\nrf = RandomForestClassifier(n_estimators=50)\n\n# # specify parameters and distributions to sample from\n# param_dist = {\"max_depth\": np.arange(1, 4),\n#               \"max_features\": sp_randint(1, 11),\n#               \"min_samples_split\": sp_randint(2, 11),\n#               \"min_samples_leaf\": sp_randint(1, 11),\n#               \"bootstrap\": [True, False],\n#               \"criterion\": [\"gini\", \"entropy\"]}\n\n# # # run randomized search\n# n_iter_search = 20\n# random_search = RandomizedSearchCV(rf, param_distributions=param_dist,\n#                                    n_iter=n_iter_search, verbose=1)\n\n# start = time.time()\n# random_search.fit(train, y_train)\n# print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n#       \" parameter settings.\" % ((time.time() - start), n_iter_search))\n# print (random_search.best_estimator_)\n\n# # use a full grid over all parameters\n# param_grid = {\"max_depth\": [3, 13, None],\n#               \"max_features\": [1, 3, 10],\n#               \"min_samples_split\": [2, 3, 10],\n#               \"min_samples_leaf\": [1, 3, 10],\n#               \"bootstrap\": [True, False],\n#               \"criterion\": [\"gini\", \"entropy\"]}\n\n# # run grid search\n# grid_search = GridSearchCV(rf, param_grid=param_grid, verbose=1)\n# start = time.time()\n# grid_search.fit(train, y_train)\n# print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n#       % (time.time() - start, len(grid_search.cv_results_['params'])))\n# print (grid_search.best_estimator_)\n\ndefault_param_rf = { 'max_depth': 3,\n                 \"max_features\":2,\n                 \"min_samples_leaf\":2,\n                 \"min_samples_split\":2,\n}\n\n# n_estimators_range = np.linspace(1, 200, 10).astype('int')\n# train_score, test_score = validation_curve(RandomForestClassifier(**default_param),\n#                                           train_data, train_labels,\n#                                           param_name='n_estimators',\n#                                           param_range=n_estimators_range,\n#                                            cv=10,\n#                                            scoring='accuracy'\n#                                           )\n\n# train_score_mean = np.mean(train_score, axis=1)\n# test_score_mean = np.mean(test_score, axis=1)\n# fig = plt.figure(figsize=(10, 6), dpi=100)\n# plt.title('Validation Score with eta=0.3')\n# plt.xlabel('Number of trees')\n# plt.ylabel('Accuracy')\n\n# plt.plot(n_estimators_range, train_score_mean, label='Training score', color='r')\n# plt.plot(n_estimators_range, test_score_mean, label='Validation score', color='g')\n# plt.legend(loc='best')\n# plt.show()\n\n\nrf = RandomForestClassifier(n_estimators=1000, **default_param_rf)\nrf.fit(train, y_train)\ny_pred = rf.predict(test)\ny_pred = (y_pred>0.5).astype('int')\nplot_confusion_matrix(y_test, y_pred, normalize=False)\nplt.show()\nplot_feature_importances(rf, feature_names=train_data.columns)\nplt.show()\n\n\nprint(\"\\tPrecision: %1.3f\" % precision_score(y_test, y_pred))\nprint(\"\\tRecall: %1.3f\" % recall_score(y_test, y_pred))\nprint(\"\\tF1: %1.3f\\n\" % f1_score(y_test, y_pred))\nprint(\"\\tAccuracy: %1.3f\" % accuracy_score(y_test, y_pred))\nprint(\"\\tROC: %1.3f\" % roc_auc_score(y_test, y_pred))","execution_count":265,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05954ca8ff4b953da8d3a5f7c634de088611e095","collapsed":true},"cell_type":"code","source":"from mlens.ensemble import SuperLearner\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score","execution_count":266,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"236ccbf7aa48f2cc49af6aa2435b108a7ede99f3"},"cell_type":"code","source":"lr = LogisticRegression()\nlr.fit(train, y_train)\ny_pred = (lr.predict(test)>0.5).astype('int')\n\nplot_confusion_matrix(y_test, y_pred, normalize=False)\nplt.show()\nprint(\"\\tPrecision: %1.3f\" % precision_score(y_test, y_pred))\nprint(\"\\tRecall: %1.3f\" % recall_score(y_test, y_pred))\nprint(\"\\tF1: %1.3f\\n\" % f1_score(y_test, y_pred))\nprint(\"\\tAccuracy: %1.3f\" % accuracy_score(y_test, y_pred))\nprint(\"\\tROC: %1.3f\" % roc_auc_score(y_test, y_pred))","execution_count":267,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05984d0f8c9aa67404f67eb92d7a6c3928ae54d4"},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\ngbc = GradientBoostingClassifier(learning_rate=0.01, n_estimators=500, **default_param_rf)\ngbc.fit(train, y_train)\ny_pred = (gbc.predict(test)>0.5).astype('int')\n\nplot_confusion_matrix(y_test, y_pred, normalize=False)\nplt.show()\nprint(\"\\tPrecision: %1.3f\" % precision_score(y_test, y_pred))\nprint(\"\\tRecall: %1.3f\" % recall_score(y_test, y_pred))\nprint(\"\\tF1: %1.3f\\n\" % f1_score(y_test, y_pred))\nprint(\"\\tAccuracy: %1.3f\" % accuracy_score(y_test, y_pred))\nprint(\"\\tROC: %1.3f\" % roc_auc_score(y_test, y_pred))","execution_count":268,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a0bdd6a681b20386435e162689eaacf57cded3da"},"cell_type":"code","source":"clf1 = lgb.LGBMClassifier(**default_params)\nclf2 = xgb.XGBClassifier(**default_param)\nclf3 = RandomForestClassifier(n_estimators=1000, **default_param_rf)\nclf4 = GradientBoostingClassifier(learning_rate=0.01, n_estimators=500, **default_param_rf)\nabc = AdaBoostClassifier(learning_rate=0.01, n_estimators=500)\nlr = LogisticRegression()\nensemble = SuperLearner(scorer=accuracy_score, random_state=43, verbose=2)\n\n# Build the first layer\nensemble.add([clf1, clf2])\n# Attach the final meta estimator\nensemble.add_meta(clf4)\n\nensemble.fit(train, y_train)\npreds = ensemble.predict(test)\nprint (\"Prediction score: %.3f\" % accuracy_score(preds, y_test))","execution_count":269,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fbefb5d754e138c3b9295e9b1522408a83e23dec"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"656b5e64eed45aba2398599ee199f03cdb889770","collapsed":true},"cell_type":"code","source":"import keras\nfrom keras.layers import Dense, Dropout\nfrom keras.models import Sequential\nfrom keras.callbacks import EarlyStopping","execution_count":270,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"089c2d0fbfca739d6b32fa62540ba4bec6d70e7d","scrolled":true},"cell_type":"code","source":"train = np.array(train)\ny_train = np.array(y_train)\ntest = np.array(test)\ny_test = np.array(y_test)\n\nbatch_size=32\nepochs=1000\n\nmodel = Sequential()\nmodel.add(Dense(train.shape[1], input_dim=train.shape[1], activation='sigmoid'))\nmodel.add(Dense(10, activation='sigmoid'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['acc'])\nes = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\nhistory = model.fit(train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(test, y_test), verbose=1, callbacks=[es])","execution_count":271,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c360fa40c47a86f83be7282ffbbfbd7c238fde9c"},"cell_type":"code","source":"# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":272,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"75f409a847bd318dabfd1f8eb9c159887d8178c1"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5052db6a9be86f2230a66493ee170b3491ebda59"},"cell_type":"markdown","source":"# Test"},{"metadata":{"trusted":true,"_uuid":"66340fa802a8f3bf71c3ca1808b2281e602c1cf8"},"cell_type":"code","source":"test_data = pd.read_csv(test_path)\ntest_data.head()","execution_count":273,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2843d1899093f047cbbb4a7966dcac95b69d5b37"},"cell_type":"code","source":"test_data.isnull().sum()","execution_count":274,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"357a867d67a54153a6c78c75906dceb3f5fb828c"},"cell_type":"code","source":"cols = ['A', 'D', 'E', 'F', 'G', 'I', 'J', 'L', 'M']\nfor item in cols:\n    test_data[item] = test_data[item].astype(\"category\").cat.codes +1\n\ntest_data.head()","execution_count":275,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"477a1a6b03ef0471c264edea338aa350bb84171e"},"cell_type":"code","source":"# cols = ['A', 'D', 'E', 'F', 'G', 'I', 'J', 'L', 'M']\n# lb = LabelEncoder()\n\n# for item in cols:\n#     test_data[item] = lb.transform(train_data[item])\n\n# test_data.head()","execution_count":276,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"00a5d8b5d787b99055367509007911be753846ca"},"cell_type":"code","source":"test_data.B = test_data.B.replace(0, np.nan)\ntest_data.N = test_data.N.replace(0, np.nan)","execution_count":277,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c52c22dcdf397f6de08152cf69fe9bfe69a89f41"},"cell_type":"code","source":"values = test_data[not_cats]\ntransformed_test_values = imputer.transform(values)\ntransformed_test_values = pd.DataFrame(data=transformed_test_values, columns=not_cats)\nprint (transformed_test_values.head())","execution_count":278,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e0a9b89d90a939c4ce48c4b32ac610ee02ca058"},"cell_type":"code","source":"ss = StandardScaler()\nvalues = transformed_test_values[not_cats]\ntransformed_test = ss.fit_transform(values)\ntransformed_test = pd.DataFrame(data=transformed_test, columns=not_cats)\ndisplay(transformed_test.head())","execution_count":279,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3623e2fe0c16b3e876712daede5149d803b080a2"},"cell_type":"code","source":"transformed_test_values.update(transformed_test)\ntransformed_test_values.head()","execution_count":280,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f8dace67d99ff903612180d1d56b11412b4338e"},"cell_type":"code","source":"test_data.update(transformed_test_values)\nprint (len(test_data))\nimport gc\ndel transformed_test_values, transformed_test\ngc.collect()","execution_count":281,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7db816ce7617c8a1a67cc63f6f0b52bbb639b989"},"cell_type":"code","source":"sub = pd.DataFrame(columns=['id', 'P'])\nsub['id'] = test_data['id']","execution_count":282,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e8e23027e09744cbcfebada39fa13c772aed01ac"},"cell_type":"code","source":"test_lgb_pred = lgb_model.predict(test_data)\ntest_lgb_pred = (test_lgb_pred>0.5).astype('int')\n# test_xgb_pred = bst.predict(xgb.DMatrix(test_data))\n# test_xgb_pred = (test_xgb_pred>0.5).astype('int')\ntest_rf_pred = rf.predict(test_data)\ntest_rf_pred = (test_rf_pred>0.5).astype('int')\ntest_ens_pred = ensemble.predict(test_data)\ntest_ens_pred = (test_ens_pred>0.5).astype('int')\ntest_gbc_pred = gbc.predict(test_data)\ntest_gbc_pred = (test_gbc_pred>0.5).astype('int')","execution_count":283,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7363cd4c1ddc2e7a4b37a3b85eb500c5572b94a0"},"cell_type":"code","source":"sub['P'] = test_lgb_pred\nsub.to_csv('submit_lgb.csv', index=False)\n# sub['P'] = test_xgb_pred\n# sub.to_csv('submit_xgb.csv', index=False)\nsub['P'] = test_rf_pred\nsub.to_csv('submit_rf.csv', index=False)\nsub['P'] = test_ens_pred\nsub.to_csv('submit_emsemble.csv', index=False)\nsub['P'] = test_gbc_pred\nsub.to_csv('submit_gbc.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ed171568f2c67cf6504667598924c1a1a7f18443"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}