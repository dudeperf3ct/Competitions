{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nimport string\nimport time\nfrom wordcloud import WordCloud,STOPWORDS\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\nfrom sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\nfrom sklearn.metrics import accuracy_score\nfrom nltk.corpus import stopwords\nfrom sklearn.naive_bayes import MultinomialNB\n\nfrom keras.utils import to_categorical\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom IPython.display import display\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport os\n\nimport spacy\nnlp = spacy.load('en')\nfrom spacy.lang.en import English\nparser = English()\n\nprint(os.listdir(\"../input\"))\nPATH = '../input/twitter-airline-sentiment'\nREAL_PATH = '../input/airline/eeb2015e-8-dataset'\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7990ae59918de1dacb23143b95a7d64944034357"},"cell_type":"code","source":"! ls {REAL_PATH}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12886173df6cfed273f3a3134f979ed2cfc68192"},"cell_type":"code","source":"train_data = pd.read_csv(f'{REAL_PATH}/train.csv')\ndisplay(train_data)\ntrain_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"621b8bc9aa33185edd15580982b0f1a0f809a33b","collapsed":true},"cell_type":"code","source":"# train_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"671650f582b0bfdd9a89712429eedb881f4bdfe0"},"cell_type":"code","source":"test_data = pd.read_csv(f'{REAL_PATH}/test.csv')\ndisplay(test_data)\ntest_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b6ca78a9e6afe828838a0cb8e61f64d4fb75c9f"},"cell_type":"code","source":"test_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06a60af0ab5e4bea17e11a7cf6697cf0eecbc85d"},"cell_type":"code","source":"all_data = pd.concat([train_data, test_data], ignore_index=True)\ndisplay(all_data)\nall_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"745bc9d59b79a5779c41f84f691543aef30ec264"},"cell_type":"code","source":"# Comment this line in case below code in uncommented\n# data = all_data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ffa0dcf8666eddbd627813bbe020e433e01bcf97"},"cell_type":"markdown","source":"# Cheat Code\n\n## Uncomment below code to train the model on whole dataset\n\n### If we train our model on this data and predict for above test dataset. It is likely to give very accurate results.\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#This is whole dataset of twitter airline sentiment from kaggle.\n\ndata = pd.read_csv(f'{PATH}/Tweets.csv')\ndisplay(data.head())\nprint (data.shape)\n\nall_data = pd.concat([all_data, data], ignore_index=True)\ndata = all_data\nprint (data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9d2dfe6b81c7aa1a314a6f25b09f9306aea8877"},"cell_type":"code","source":"data.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2fa8714d8242a3e2e20f8a2d2fb9b458017d250"},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f285bcff2bba8ceb8a4e5609713f293e2ad2fc49"},"cell_type":"code","source":"data = data.dropna(subset=['airline'])\ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"901233e886d305f5c0eebf26c36e798771bb3c38"},"cell_type":"code","source":"sns.countplot(y='airline', hue='airline_sentiment', data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1052a17317643baff0e50a60545fd17142306ae7"},"cell_type":"code","source":"sns.countplot(y='negativereason', data=data)\nprint (data.negativereason.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ceef96a5f78b7e7a6b94cdf15988247627d136f6"},"cell_type":"code","source":"sns.countplot(x='airline', data=data)\nprint (data.airline.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8919aa703bb8cc2a7ee470280f0a38b4525d4375"},"cell_type":"code","source":"# data = data[['airline_sentiment', 'text']]\ndisplay(data.head())\nprint (data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c48f53cb742aedc99541a478326afd5af75a824","collapsed":true},"cell_type":"code","source":"#visualization using wordcloud for the neutral tweets\ndf=data[data['airline_sentiment']=='neutral']\nwords = ' '.join(df['text'])\nvalid_word = \" \".join([word for word in words.split()\n                            if 'http' not in word\n                                and not word.startswith('@')\n                                and word != 'RT'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e16fe586e9c9f1e73e1575c1447be7ac04be216"},"cell_type":"code","source":"wordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                      width=3000,\n                      height=2500\n                     ).generate(valid_word)\nplt.figure(1,figsize=(12, 12))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"369034611ba9a9d7d96d89cb08aa1a060de25f0a"},"cell_type":"code","source":"#visualization using wordcloud for the positive tweets\ndf=data[data['airline_sentiment']=='positive']\nwords = ' '.join(df['text'])\nvalid_word = \" \".join([word for word in words.split()\n                            if 'http' not in word\n                                and not word.startswith('@')\n                                and word != 'RT'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a7137ca9577daaf636d01e22c594f617be154cfe"},"cell_type":"code","source":"wordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                      width=3000,\n                      height=2500\n                     ).generate(valid_word)\nplt.figure(1,figsize=(12, 12))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8d56602dc778674d9a7f124727802c3ade8317b","collapsed":true},"cell_type":"code","source":"#visualization using wordcloud for the negative tweets\ndf=data[data['airline_sentiment']=='negative']\nwords = ' '.join(df['text'])\nvalid_word = \" \".join([word for word in words.split()\n                            if 'http' not in word\n                                and not word.startswith('@')\n                                and not word.startswith('$')\n                                and word != 'RT'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"52cccd9b421b8662d2222086246862af65054f6a"},"cell_type":"code","source":"wordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                      width=3000,\n                      height=2500\n                     ).generate(valid_word)\nplt.figure(1,figsize=(12, 12))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44e480859f1b8bccd2e9fa0535119b9500fa3462"},"cell_type":"code","source":"sns.countplot(x='airline_sentiment', data=data)\nprint (data.airline_sentiment.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59b53c7e7dc9f9627f4ad55166d1497863529ee3"},"cell_type":"code","source":"sentiment, tweets = data['airline_sentiment'], data['text']\nprint (tweets[:5])\nprint (sentiment[:5])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a045b75dce2cde772af3f09d85e51528264eb70a"},"cell_type":"markdown","source":"# Text Preprocessing\n\n## Steps\n1. Decode into utf-8 format and convert all to lower\n1. Remove url, @ and #(hastags)\n1. Remove punctuations, multiple whitespaces and stop words"},{"metadata":{"trusted":true,"_uuid":"f79ece7e5eabf53a6ec0baae80dac215d5424f21"},"cell_type":"code","source":"# Remove mentions @, hastags #, dollar $ or urls if present and convert it to lower\ndef remove_url_tags(tweet):\n    tweet = tweet.lower()\n    tweet = re.sub(r'@([^\\s]+)', r'\\1', tweet)\n    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n    tweet = re.sub(r'$([^\\s]+)', '', tweet)\n    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', ' ', tweet)\n    tweet = tweet.strip('\\'\"')\n    return tweet\nsample = '@VirginAmerica What @dhepburn said.' \nprint (sample)\nprint (remove_url_tags(sample))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"685f3459ede8abab068344a2d5c92e6765bfe352"},"cell_type":"code","source":"# Remove digits, double whitespaces, letters_only\ndef remove_d_dws(tweet):\n    tweet = re.sub(\"[^a-zA-Z]\", ' ',tweet) \n    tweet = re.sub(\"\\d+\", ' ', tweet)\n    tweet = re.sub('[\\s]+', ' ', tweet)\n    return (\" \".join(tweet.split())) #remove any trailing whitespaces\n\nsample = '@USAirways  last 2 times I checked a bags they were severally damaged.   No one answers the baggage call line for status?  #chairmanlove' \nprint (sample)\nprint (remove_d_dws(remove_url_tags(sample)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"62b116f3103e4adc5f444f97f23116bb01812bcc"},"cell_type":"code","source":"#Preprocess all tweets\ndef preprocess(tweet):\n    tweet = remove_url_tags(tweet)\n    tweet = remove_d_dws(tweet)\n    return tweet\n\ndata['clean_text'] = data['text'].apply(lambda x : preprocess(x))\ntrain_clean_text = []\nfor tweets in data['clean_text']:\n    train_clean_text.append(tweets)\nprint (train_clean_text[:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a3c60b585569a61d13358c224602523f0d0629a8"},"cell_type":"code","source":"# Remove stopwords, punctuation and tokenize tweets\nSTOPLIST = set(stopwords.words('english') + list(ENGLISH_STOP_WORDS))\nSYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-\", \"...\", \"”\", \"”\"]\n\ndef tokenizeText(tweet):\n    tokens = parser(tweet)\n    lemmas = []\n    for tok in tokens:\n        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_)\n    tokens = lemmas\n    tokens = [tok for tok in tokens if tok not in STOPLIST]\n    tokens = [tok for tok in tokens if tok not in SYMBOLS]\n    return tokens  ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2c7152f0c3578014943fbb2509a8e5131d1ecc1"},"cell_type":"markdown","source":"# Feature Vectorizer\n\nThere are 3 ways to create feature vector for tokens\n1.  Bag of model with Count Vectorizer n-gram\n1. Bag of model with Tf-idf Vectorizer n-gram\n1. Pretrained Embeddings (custom, word2vec, glove, fasttext)"},{"metadata":{"trusted":true,"_uuid":"0301802d378b36a4405584edcbc996a1b4b3139f"},"cell_type":"code","source":"#Comment this block while training on all data\n\ntrain_data = data[:3338]\ntest_data = data[3338:3709]\ndata = data[3709:]\ndisplay(train_data[:3])\ndisplay(test_data[:3])\ndisplay(data[:3])\nprint (train_data.shape)\nprint (test_data.shape)\nprint (data.shape)\n# sentiment = train_data['airline_sentiment']\n# train_clean_text = train_data['clean_text']\nsentiment = data['airline_sentiment']\ntrain_clean_text = data['clean_text']\ntest_clean_text = test_data['clean_text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af92638c20cffa7d0cbbcdcedec3a315a4a30410"},"cell_type":"code","source":"# Label Encoder and One-hot Encoding\nlb = LabelEncoder()\nsentiment = lb.fit_transform(sentiment)\n# sentiment = to_categorical(sentiment, num_classes=3)\nprint (sentiment.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"245426b0f60d86cff4a1aa510d3e39601d69eb9c","collapsed":true},"cell_type":"code","source":"vectorizer = CountVectorizer(tokenizer=tokenizeText, ngram_range=(1,1), analyzer='word')\ncv_features = vectorizer.fit_transform(train_clean_text).toarray()\ncv_test_features = vectorizer.transform(test_clean_text).toarray()      #Comment this line while training on all data\ncv_labels = sentiment\nprint (cv_features.shape)\nprint (cv_labels.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb57a06c4d0584a409835b36e9b7eae08c662964","collapsed":true},"cell_type":"code","source":"tfidf = TfidfVectorizer(tokenizer=tokenizeText, sublinear_tf=True, min_df=5, norm='l2', ngram_range=(1, 2), \n                        stop_words='english', analyzer='word')\ntfidf_features = tfidf.fit_transform(train_clean_text).toarray()\ntfidf_test_features = tfidf.transform(test_clean_text).toarray()\ntfidf_labels = sentiment\nprint (tfidf_features.shape)\nprint (tfidf_labels.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a18d112ac8442fdec7103d8a3eb9957da29eb22","collapsed":true},"cell_type":"code","source":"print (sorted(vectorizer.vocabulary_.items(), key=lambda cv:cv[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8afbc7b2dfc8097e4c6dbcde7ce0df2c43391221","collapsed":true},"cell_type":"code","source":"print (sorted(tfidf.vocabulary_.items(), key=lambda tfif:tfif[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1d9ecf86e7eaa7a2652d10a5a14b60bdb2ae4a49"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac3d8391bb8a9dbe97ce053893112703c4a05572"},"cell_type":"markdown","source":"# Classifier Models"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3d754c0d0922ea9f76f3548a0645bae4747ffd42"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2788c0d0b316c777e612ddf380dcd8e5ff7c4acc","collapsed":true},"cell_type":"code","source":"train_x, val_x, train_y, val_y = train_test_split(cv_features, cv_labels, test_size=0.2,random_state=42)\nprint ('Training Shape:', train_x.shape, train_y.shape)\nprint ('Validation Shape:', val_x.shape, val_y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"91cefbd0bfd3db4f91cb4d953ba707641c11716b"},"cell_type":"code","source":"Classifiers = [\n    LogisticRegression(C=0.000000001, max_iter=400),\n    KNeighborsClassifier(3),\n    GaussianNB(),\n#     SVC(kernel=\"rbf\", C=0.025, probability=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(n_estimators=500),\n    AdaBoostClassifier()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5387f5e85a3d6c214983378776efcf5c6988025","collapsed":true},"cell_type":"code","source":"Accuracy=[]\nModel=[]\nfor classifier in Classifiers:\n    clf = classifier.fit(train_x, train_y)\n    pred = clf.predict(val_x)\n    accuracy = accuracy_score(val_y, pred)\n    Accuracy.append(accuracy)\n    Model.append(classifier.__class__.__name__)\n    print('Accuracy of '+ classifier.__class__.__name__+' is '+ str(accuracy))  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af5cf6de98e07e43ed43797668f7c14a244957ee","collapsed":true},"cell_type":"code","source":"Index = [1,2,3,4,5,6]\nplt.bar(Index,Accuracy)\nplt.xticks(Index, Model, rotation=45)\nplt.ylabel('Accuracy')\nplt.xlabel('Model')\nplt.title('Accuracies of Models')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5973d29d55015c2af6b0ddfae4eee0f4ac770964","collapsed":true},"cell_type":"code","source":"for clf in Classifiers:\n    test_pred = clf.predict(cv_test_features)\n    \n    sub = pd.DataFrame()\n    sub['tweet_id'] = test_data['tweet_id']\n    sub['airline_sentiment'] = lb.inverse_transform(test_pred)\n    sub.to_csv('submit_new_'+clf.__class__.__name__+'.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f0179dca7f526c9f327955bc748d8de98081b003","collapsed":true},"cell_type":"code","source":"# !ls\n# sa = pd.read_csv('/kaggle/working/submitRandomForestClassifier.csv')\n# sa.head(30)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"01546d5476c16c58da4f2388f10f6b16cef77120"},"cell_type":"markdown","source":"# Neural Network Approach"},{"metadata":{"trusted":true,"_uuid":"5ee4662be06763d4c19aed191f1ec6c97ad99e99","collapsed":true},"cell_type":"code","source":"import keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, LSTM\nfrom keras.layers import Embedding, Conv1D, GlobalMaxPooling1D\nfrom keras.callbacks import EarlyStopping","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b61273d66b21121d88af5f6e317a2989b60fb3d"},"cell_type":"markdown","source":"# Spacy Pretrained English Model\n\n###  Replacing all the tokens with the embedding of the english words from model"},{"metadata":{"trusted":true,"_uuid":"81163f5ef16960b23795a259ebb695f853a1fc92","collapsed":true},"cell_type":"code","source":"#gets the average wordvec\nvec = [doc.vector for doc in nlp.pipe(data['clean_text'], n_threads=50)]\n# data['wordvec] = list(np.array(vec))\nprint (np.array(vec).shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd3277b2ee059b820c16e03c6e1160d60533f04d"},"cell_type":"markdown","source":"# Word2Vec\n\n### Training a custom word2vec model for embeddings using gensim. As the size of dataset is very small, there is no effect on performance."},{"metadata":{"trusted":true,"_uuid":"f89f0d179ff205cb76748c4a9f841d2d252a681d","collapsed":true},"cell_type":"code","source":"import gensim\n\ndef read_input():\n    for line in data['clean_text']:\n        yield gensim.utils.simple_preprocess(line)\n        \ndocuments = list(read_input())\n        \nmodel_word2vec = gensim.models.Word2Vec(documents, size=150, window=3, min_count=2, workers=10)\n\nmodel_word2vec.train(documents, total_examples=len(documents), epochs=10)\nmodel_word2vec.save('senti_word2vec.vec')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3088bebfad31060cb655fb6edf6254223c881320","collapsed":true},"cell_type":"code","source":"model_word2vec.wv.most_similar(positive='late')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"acc65e7cb3b4ed67b4e167b43e4db6cf20e275f7"},"cell_type":"markdown","source":"# FastText"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"38be2b77444f2fef0c6f649bc01ec50706993118"},"cell_type":"code","source":"# import gensim\n\n# def read_input():\n#     for line in data['clean_text']:\n#         yield gensim.utils.simple_preprocess(line)\n        \n# documents = list(read_input())\n        \n# model_fasttext = gensim.models.FastText(documents, size=150, window=3, min_count=2, workers=10)\n\n# model_fasttext.train(documents, total_examples=len(documents), epochs=10)\n# model_fasttext.save('senti_fasttext.vec')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d360450cecfde90704de0ca5a047308af5a98a9","collapsed":true},"cell_type":"code","source":"max_features = 5000\ntokenizer = Tokenizer(num_words=max_features, split=' ')\n# tokenizer.fit_on_texts(train_data['clean_text'].values)\ntokenizer.fit_on_texts(data['clean_text'].values)                     #Uncomment this for training whole dataset\n# X = tokenizer.texts_to_sequences(train_data['clean_text'].values)       #Uncomment this for training whole dataset\ntest_X = tokenizer.texts_to_sequences(test_data['clean_text'].values)   #Comment this for training whole dataset\nX = tokenizer.texts_to_sequences(data['clean_text'].values)           #Uncomment this for training whole dataset\nX = pad_sequences(X)\ntest_X = pad_sequences(test_X, maxlen=X.shape[1])                       #Comment this for training whole dataset\nprint (X.shape, test_X.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32bf4ff0e0620c76cdfb4f8cab28f6a163f9810f","collapsed":true},"cell_type":"code","source":"# X = np.array(vec)                                        #Pretrained embedding of spacy english model\nY = pd.get_dummies(data['airline_sentiment'].values)     #Uncomment this for training whole dataset\n# Y = pd.get_dummies(train_data['airline_sentiment'].values)\n\n\ntrain_X, val_X, train_Y, val_Y = train_test_split(X, Y, test_size=0.2,random_state=42)\nprint ('Training Shape:', train_X.shape, train_Y.shape)\nprint ('Validation Shape:', val_X.shape, val_Y.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6a4a74734f210e07090ab5b18211ed9ce818c69"},"cell_type":"markdown","source":"# Models\n\n1.  LSTM\n1. CNN 1D"},{"metadata":{"trusted":true,"_uuid":"002e6b34fe608d52d3bf55c1a4760c4a34332543","collapsed":true},"cell_type":"code","source":"embed_dim = 128\nout = 196\nbatch_size = 64\nepochs = 10\nnum_classes = 3\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, embed_dim, input_length=X.shape[1]))\nmodel.add(Dropout(0.5))\nmodel.add(LSTM(out, dropout=0.5, recurrent_dropout=0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f89a0f9da1ee652193ce6b504ed67b70afbe0a98","collapsed":true},"cell_type":"code","source":"es = EarlyStopping(monitor='val_acc', patience=3, verbose=1)\nhistory = model.fit(train_X, train_Y,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    validation_data=(val_X, val_Y),\n                    callbacks=[es])\n\n# Score trained model.\nscores = model.evaluate(val_X, val_Y, verbose=1)\nprint('Validation loss:', scores[0])\nprint('Validation accuracy:', scores[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b8a4f3d1f516082c26e064483ca47462f981095","collapsed":true},"cell_type":"code","source":"# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"564b1c180be2605e37cda72b4cdec6e0f2b37cee","collapsed":true},"cell_type":"code","source":"test_pred = model.predict(test_X)\nsub = pd.DataFrame()\nsub['tweet_id'] = test_data['tweet_id']\nsub['airline_sentiment'] = lb.inverse_transform(np.argmax(test_pred, axis=1))\nsub.to_csv('submit_new_lstm.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eec4f1440aec8d2bb799fc202d9a4173d463e390","collapsed":true},"cell_type":"code","source":"# CNN\nmax_features = 5000\nbatch_size = 128\nembedding_dims = 50\nfilters = 250\nkernel_size = 3\nhidden_dims = 250\nepochs = 10\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, embedding_dims, input_length=X.shape[1]))\nmodel.add(Dropout(0.5))\nmodel.add(Conv1D(filters, kernel_size,\n                 padding='valid', activation='relu', strides=1))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dense(hidden_dims))\nmodel.add(Dropout(0.5))\nmodel.add(Activation('relu'))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"002e381702482cdc1cc3da2b5e238368a42c6120","collapsed":true},"cell_type":"code","source":"es = EarlyStopping(monitor='val_acc', patience=3, verbose=1)\nhistory = model.fit(train_X, train_Y,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    validation_data=(val_X, val_Y),\n                    callbacks=[es])\n\n# Score trained model.\nscores = model.evaluate(val_X, val_Y, verbose=1)\nprint('Validation loss:', scores[0])\nprint('Validation accuracy:', scores[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0df0761372de58ebb6717ad4a714b82afbae8d8d","collapsed":true},"cell_type":"code","source":"# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86e4f49e9b0d45901c3a81c3afe2b0ad23d4d77e","collapsed":true},"cell_type":"code","source":"test_pred = model.predict(test_X)\nsub = pd.DataFrame()\nsub['tweet_id'] = test_data['tweet_id']\nsub['airline_sentiment'] = lb.inverse_transform(np.argmax(test_pred, axis=1))\nsub.to_csv('submit_new_cnn1d.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2477da6d86f6d5372b7182c663437ab7c9b894f5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}